{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6564c7c-7467-4430-81e4-49c0af5bac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from typing import Dict, Text\n",
    "\n",
    "#data import\n",
    "impressions = pd.read_csv(\n",
    "    \"TransformedData.csv\",\n",
    "    header = None,\n",
    "    names= ['user_id','timestamp','history','category','subcategory','title','next_item']\n",
    "    ) \n",
    "\n",
    "impressions = impressions.drop(columns=['user_id','timestamp','title'])\n",
    "\n",
    "news_data = pd.read_table(\"news.tsv\",\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'next_id', 'next_category', 'next_subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "\n",
    "news_data = news_data.drop(columns=['title', 'abstract','url', 'title_entities','abstract_entities'])\n",
    "news_data = news_data.drop_duplicates('next_id')\n",
    "\n",
    "history = impressions[\"history\"].map(lambda x: literal_eval(x)).tolist()\n",
    "category = impressions[\"category\"].map(lambda x: literal_eval(x)).tolist()\n",
    "subcategory = impressions[\"subcategory\"].map(lambda x: literal_eval(x)).tolist()\n",
    "next_id = impressions[\"next_item\"].map(lambda x: literal_eval(x)[0])\n",
    "next_category = impressions[\"next_item\"].map(lambda x: literal_eval(x)[1])\n",
    "next_subcategory = impressions[\"next_item\"].map(lambda x: literal_eval(x)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6244ef-c76c-4e5e-88c2-d786c6333d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = tf.ragged.constant(history, dtype=tf.string)\n",
    "category = tf.ragged.constant(category, dtype=tf.string)\n",
    "subcategory = tf.ragged.constant(subcategory, dtype=tf.string)\n",
    "next_id = tf.constant(next_id, dtype=tf.string)\n",
    "next_category = tf.constant(next_category, dtype=tf.string)\n",
    "next_subcategory = tf.constant(next_subcategory, dtype=tf.string)\n",
    "\n",
    "news_dict = {name: np.array(value) for name, value in news_data.items()}\n",
    "impressions_dict = {\n",
    "    \"history\" : history,\n",
    "    \"category\" : category,\n",
    "    \"subcategory\" : subcategory,\n",
    "    \"next_id\" : next_id,\n",
    "    \"next_category\" : next_category,\n",
    "    \"next_subcategory\" : next_subcategory,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c447f92-bff8-4864-95e9-2ead636f2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_ds = tf.data.Dataset.from_tensor_slices(news_dict)\n",
    "impressions_ds = tf.data.Dataset.from_tensor_slices(impressions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c7b24c-0278-4deb-b1ce-b8cd1e05a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabularies\n",
    "news_id_vocabulary = np.unique(np.concatenate(list(news_ds.batch(1_000).map(lambda x: x[\"next_id\"]))))\n",
    "news_category_vocabulary = np.unique(np.concatenate(list(news_ds.batch(1_000).map(lambda x: x[\"next_category\"]))))\n",
    "news_subcategory_vocabulary = np.unique(np.concatenate(list(news_ds.batch(1_000).map(lambda x: x[\"next_subcategory\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b439cbc7-701f-4ab8-bd4f-c8cac7385589",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_ds = news_ds.map(lambda x: {\n",
    "    \"next_id\": x['next_id'],\n",
    "    \"next_category\": x['next_category'],\n",
    "    \"next_subcategory\": x['next_subcategory'],\n",
    "})\n",
    "\n",
    "impressions_ds = impressions_ds.map(lambda x: {\n",
    "    \"history\" : x[\"history\"],\n",
    "    \"category\" : x[\"category\"],\n",
    "    \"subcategory\" : x[\"subcategory\"],\n",
    "    \"next_id\" : x[\"next_id\"],\n",
    "    \"next_category\" : x[\"next_category\"],\n",
    "    \"next_subcategory\" : x[\"next_subcategory\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba9651fc-36d2-4c9e-b4b2-422dbfd81f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'args_1:0' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'args_0:0' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'args_2:0' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension=128\n",
    "learning_rate=0.1\n",
    "epochs=100\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Create History Model\n",
    "        self.history_model = tf.keras.Sequential()\n",
    "        self.history_model._name = \"user_history\"\n",
    "        self.history_model.add(tf.keras.layers.StringLookup(vocabulary=news_id_vocabulary, mask_token=None))\n",
    "        self.history_model.add(tf.keras.layers.Embedding(len(news_id_vocabulary)+1, embedding_dimension))\n",
    "        self.history_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "\n",
    "        #Create Category Model\n",
    "        self.category_model = tf.keras.Sequential()\n",
    "        self.category_model._name = \"user_category\"\n",
    "        self.category_model.add(tf.keras.layers.StringLookup(vocabulary=news_category_vocabulary, mask_token=None))\n",
    "        self.category_model.add(tf.keras.layers.Embedding(len(news_category_vocabulary)+1, embedding_dimension))\n",
    "        self.category_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "\n",
    "        #Create SubCategory Model\n",
    "        self.subcategory_model = tf.keras.Sequential()\n",
    "        self.subcategory_model._name = \"user_subcategory\"\n",
    "        self.subcategory_model.add(tf.keras.layers.StringLookup(vocabulary=news_subcategory_vocabulary, mask_token=None))\n",
    "        self.subcategory_model.add(tf.keras.layers.Embedding(len(news_subcategory_vocabulary)+1, embedding_dimension))\n",
    "        self.subcategory_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "\n",
    "    def call(self, features) -> tf.Tensor:\n",
    "        return tf.concat([\n",
    "            self.history_model(features[\"history\"]),\n",
    "            self.category_model(features[\"category\"]),\n",
    "            self.subcategory_model(features[\"subcategory\"]),\n",
    "        ], axis = 1)\n",
    "    \n",
    "class NewsModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ID_model\n",
    "        self.NewsId_model = tf.keras.Sequential()\n",
    "        self.NewsId_model._name = \"news_id\"\n",
    "        self.NewsId_model.add(tf.keras.layers.StringLookup(vocabulary=news_id_vocabulary, mask_token=None))\n",
    "        self.NewsId_model.add(tf.keras.layers.Embedding(len(news_id_vocabulary) +1, embedding_dimension))\n",
    "        \n",
    "        # category model\n",
    "        self.news_category_model = tf.keras.Sequential()\n",
    "        self.news_category_model._name = \"news_category\"\n",
    "        self.news_category_model.add(tf.keras.layers.StringLookup(vocabulary=news_category_vocabulary, mask_token=None))\n",
    "        self.news_category_model.add(tf.keras.layers.Embedding(len(news_category_vocabulary) +1, embedding_dimension))\n",
    "        \n",
    "        # subcategory model\n",
    "        self.news_subcategory_model = tf.keras.Sequential()\n",
    "        self.news_subcategory_model._name = \"news_subcategory\"\n",
    "        self.news_subcategory_model.add(tf.keras.layers.StringLookup(vocabulary=news_subcategory_vocabulary, mask_token=None))\n",
    "        self.news_subcategory_model.add(tf.keras.layers.Embedding(len(news_subcategory_vocabulary) +1, embedding_dimension))\n",
    "\n",
    "    def call(self, features) -> tf.Tensor:\n",
    "        return tf.concat([\n",
    "            self.NewsId_model(features[\"next_id\"]),\n",
    "            self.news_category_model(features[\"next_category\"]),\n",
    "            self.news_subcategory_model(features[\"next_subcategory\"]),\n",
    "        ], axis = 1)\n",
    "    \n",
    "class Model(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query_model = tf.keras.Sequential([\n",
    "            UserModel(),\n",
    "\n",
    "        ])\n",
    "        \n",
    "        self.query_model._name = \"query\"\n",
    "        \n",
    "        self.candidate_model = tf.keras.Sequential([\n",
    "            NewsModel(),\n",
    "        ])\n",
    "        \n",
    "        self.candidate_model._name = \"candidate\"\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates= news_ds.batch(1024).map(self.candidate_model),\n",
    "                ),\n",
    "            name = \"retrival_task\"\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        candidate_embedding = self.candidate_model({\n",
    "            \"next_id\": features[\"next_id\"],\n",
    "            \"next_category\":features[\"next_category\"],\n",
    "            \"next_subcategory\": features[\"next_subcategory\"],\n",
    "        })\n",
    "        query_embedding = self.query_model({\n",
    "            \"history\": features[\"history\"],\n",
    "            \"category\":features[\"category\"],\n",
    "            \"subcategory\": features[\"subcategory\"],\n",
    "        })\n",
    "        return self.task(query_embedding, candidate_embedding, compute_metrics=not training)\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ce9263-8de8-4585-85b0-1c4ae0261ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'history': <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=string>, 'category': <tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=string>, 'subcategory': <tf.Tensor 'IteratorGetNext:5' shape=(None, None) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'history': <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=string>, 'category': <tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=string>, 'subcategory': <tf.Tensor 'IteratorGetNext:5' shape=(None, None) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "13/13 [==============================] - 12s 254ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 121533.7176 - regularization_loss: 0.0000e+00 - total_loss: 121533.7176\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 2s 110ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 93162.0050 - regularization_loss: 0.0000e+00 - total_loss: 93162.0050\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 2s 124ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 90672.6825 - regularization_loss: 0.0000e+00 - total_loss: 90672.6825\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 89266.1691 - regularization_loss: 0.0000e+00 - total_loss: 89266.1691\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 87724.4431 - regularization_loss: 0.0000e+00 - total_loss: 87724.4431\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 85880.3359 - regularization_loss: 0.0000e+00 - total_loss: 85880.3359\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 83761.4141 - regularization_loss: 0.0000e+00 - total_loss: 83761.4141\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 81733.9074 - regularization_loss: 0.0000e+00 - total_loss: 81733.9074\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 2s 136ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 79507.1267 - regularization_loss: 0.0000e+00 - total_loss: 79507.1267\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 77212.2450 - regularization_loss: 0.0000e+00 - total_loss: 77212.2450\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 1s 93ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 75345.1730 - regularization_loss: 0.0000e+00 - total_loss: 75345.1730\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 73158.3510 - regularization_loss: 0.0000e+00 - total_loss: 73158.3510\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 71538.7009 - regularization_loss: 0.0000e+00 - total_loss: 71538.7009\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 69381.9475 - regularization_loss: 0.0000e+00 - total_loss: 69381.9475\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 67585.5073 - regularization_loss: 0.0000e+00 - total_loss: 67585.5073\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 65988.0028 - regularization_loss: 0.0000e+00 - total_loss: 65988.0028\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 64477.3256 - regularization_loss: 0.0000e+00 - total_loss: 64477.3256\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 62952.8990 - regularization_loss: 0.0000e+00 - total_loss: 62952.8990\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 61736.3619 - regularization_loss: 0.0000e+00 - total_loss: 61736.3619\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 60438.4774 - regularization_loss: 0.0000e+00 - total_loss: 60438.4774\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 59256.3577 - regularization_loss: 0.0000e+00 - total_loss: 59256.3577\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 58118.4311 - regularization_loss: 0.0000e+00 - total_loss: 58118.4311\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 57316.3253 - regularization_loss: 0.0000e+00 - total_loss: 57316.3253\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 56348.7849 - regularization_loss: 0.0000e+00 - total_loss: 56348.7849\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 2s 139ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 55473.3292 - regularization_loss: 0.0000e+00 - total_loss: 55473.3292\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 54663.8915 - regularization_loss: 0.0000e+00 - total_loss: 54663.8915\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 53993.2921 - regularization_loss: 0.0000e+00 - total_loss: 53993.2921\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 1s 94ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 53470.7017 - regularization_loss: 0.0000e+00 - total_loss: 53470.7017\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 1s 104ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 52969.7812 - regularization_loss: 0.0000e+00 - total_loss: 52969.7812\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 52517.0879 - regularization_loss: 0.0000e+00 - total_loss: 52517.0879\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 1s 99ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 51974.4400 - regularization_loss: 0.0000e+00 - total_loss: 51974.4400\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 1s 104ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 51610.3209 - regularization_loss: 0.0000e+00 - total_loss: 51610.3209\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 1s 106ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 51170.4342 - regularization_loss: 0.0000e+00 - total_loss: 51170.4342\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 1s 104ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 51192.8644 - regularization_loss: 0.0000e+00 - total_loss: 51192.8644\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 1s 101ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 50971.0522 - regularization_loss: 0.0000e+00 - total_loss: 50971.0522\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 50615.0929 - regularization_loss: 0.0000e+00 - total_loss: 50615.0929\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 50397.3619 - regularization_loss: 0.0000e+00 - total_loss: 50397.3619\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 1s 100ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 50076.6088 - regularization_loss: 0.0000e+00 - total_loss: 50076.6088\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 49758.5552 - regularization_loss: 0.0000e+00 - total_loss: 49758.5552\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 49611.3457 - regularization_loss: 0.0000e+00 - total_loss: 49611.3457\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 49343.1431 - regularization_loss: 0.0000e+00 - total_loss: 49343.1431\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 1s 100ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 49055.0737 - regularization_loss: 0.0000e+00 - total_loss: 49055.0737\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 48847.2397 - regularization_loss: 0.0000e+00 - total_loss: 48847.2397\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 48622.7372 - regularization_loss: 0.0000e+00 - total_loss: 48622.7372\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 1s 101ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 48394.8680 - regularization_loss: 0.0000e+00 - total_loss: 48394.8680\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 48173.7243 - regularization_loss: 0.0000e+00 - total_loss: 48173.7243\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 48164.3170 - regularization_loss: 0.0000e+00 - total_loss: 48164.3170\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 47894.5502 - regularization_loss: 0.0000e+00 - total_loss: 47894.5502\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 47817.5929 - regularization_loss: 0.0000e+00 - total_loss: 47817.5929\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 47597.0511 - regularization_loss: 0.0000e+00 - total_loss: 47597.0511\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 47498.3728 - regularization_loss: 0.0000e+00 - total_loss: 47498.3728\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 47330.2662 - regularization_loss: 0.0000e+00 - total_loss: 47330.2662\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 47177.9989 - regularization_loss: 0.0000e+00 - total_loss: 47177.9989\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 46992.0474 - regularization_loss: 0.0000e+00 - total_loss: 46992.0474\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 46850.5502 - regularization_loss: 0.0000e+00 - total_loss: 46850.5502\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 46573.5943 - regularization_loss: 0.0000e+00 - total_loss: 46573.5943\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 46372.6596 - regularization_loss: 0.0000e+00 - total_loss: 46372.6596\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 1s 103ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 46172.0622 - regularization_loss: 0.0000e+00 - total_loss: 46172.0622\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 1s 104ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 45995.0929 - regularization_loss: 0.0000e+00 - total_loss: 45995.0929\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 1s 102ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 45783.1384 - regularization_loss: 0.0000e+00 - total_loss: 45783.1384\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 1s 109ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 45531.0826 - regularization_loss: 0.0000e+00 - total_loss: 45531.0826\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 45317.7955 - regularization_loss: 0.0000e+00 - total_loss: 45317.7955\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 1s 103ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 45187.1853 - regularization_loss: 0.0000e+00 - total_loss: 45187.1853\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 45022.8354 - regularization_loss: 0.0000e+00 - total_loss: 45022.8354\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 44825.3295 - regularization_loss: 0.0000e+00 - total_loss: 44825.3295\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 1s 99ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 44624.6364 - regularization_loss: 0.0000e+00 - total_loss: 44624.6364\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 44450.8892 - regularization_loss: 0.0000e+00 - total_loss: 44450.8892\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 44297.3292 - regularization_loss: 0.0000e+00 - total_loss: 44297.3292\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 44147.3331 - regularization_loss: 0.0000e+00 - total_loss: 44147.3331\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 44041.4713 - regularization_loss: 0.0000e+00 - total_loss: 44041.4713\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43864.1674 - regularization_loss: 0.0000e+00 - total_loss: 43864.1674\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43791.5340 - regularization_loss: 0.0000e+00 - total_loss: 43791.5340\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 1s 102ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43700.0996 - regularization_loss: 0.0000e+00 - total_loss: 43700.0996\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 1s 102ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43651.0935 - regularization_loss: 0.0000e+00 - total_loss: 43651.0935\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 1s 103ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43585.0338 - regularization_loss: 0.0000e+00 - total_loss: 43585.0338\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43544.5709 - regularization_loss: 0.0000e+00 - total_loss: 43544.5709\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 1s 99ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43481.0167 - regularization_loss: 0.0000e+00 - total_loss: 43481.0167\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 1s 99ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43438.7182 - regularization_loss: 0.0000e+00 - total_loss: 43438.7182\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43413.2508 - regularization_loss: 0.0000e+00 - total_loss: 43413.2508\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43347.0971 - regularization_loss: 0.0000e+00 - total_loss: 43347.0971\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43331.5857 - regularization_loss: 0.0000e+00 - total_loss: 43331.5857\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 1s 99ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43242.7196 - regularization_loss: 0.0000e+00 - total_loss: 43242.7196\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 1s 100ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43224.4604 - regularization_loss: 0.0000e+00 - total_loss: 43224.4604\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 1s 99ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43143.9665 - regularization_loss: 0.0000e+00 - total_loss: 43143.9665\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 1s 99ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43108.5193 - regularization_loss: 0.0000e+00 - total_loss: 43108.5193\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43060.1278 - regularization_loss: 0.0000e+00 - total_loss: 43060.1278\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 43006.8778 - regularization_loss: 0.0000e+00 - total_loss: 43006.8778\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42909.5095 - regularization_loss: 0.0000e+00 - total_loss: 42909.5095\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42862.8465 - regularization_loss: 0.0000e+00 - total_loss: 42862.8465\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 1s 101ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42769.7405 - regularization_loss: 0.0000e+00 - total_loss: 42769.7405\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 1s 99ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42730.1242 - regularization_loss: 0.0000e+00 - total_loss: 42730.1242\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 1s 98ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42653.2380 - regularization_loss: 0.0000e+00 - total_loss: 42653.2380\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42589.9113 - regularization_loss: 0.0000e+00 - total_loss: 42589.9113\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42531.5363 - regularization_loss: 0.0000e+00 - total_loss: 42531.5363\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 1s 95ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42484.5700 - regularization_loss: 0.0000e+00 - total_loss: 42484.5700\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42427.6468 - regularization_loss: 0.0000e+00 - total_loss: 42427.6468\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42359.8181 - regularization_loss: 0.0000e+00 - total_loss: 42359.8181\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42275.6504 - regularization_loss: 0.0000e+00 - total_loss: 42275.6504\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 1s 96ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42254.4434 - regularization_loss: 0.0000e+00 - total_loss: 42254.4434\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 1s 97ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42208.2285 - regularization_loss: 0.0000e+00 - total_loss: 42208.2285\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'history': <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=string>, 'category': <tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=string>, 'subcategory': <tf.Tensor 'IteratorGetNext:5' shape=(None, None) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "10/10 [==============================] - 8s 458ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0012 - factorized_top_k/top_5_categorical_accuracy: 0.0120 - factorized_top_k/top_10_categorical_accuracy: 0.0181 - factorized_top_k/top_50_categorical_accuracy: 0.0617 - factorized_top_k/top_100_categorical_accuracy: 0.0976 - loss: 12722.4274 - regularization_loss: 0.0000e+00 - total_loss: 12722.4274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0012000000569969416,\n",
       " 0.012000000104308128,\n",
       " 0.01810000091791153,\n",
       " 0.0617000013589859,\n",
       " 0.09759999811649323,\n",
       " 10042.166015625,\n",
       " 0,\n",
       " 10042.166015625]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train Model\n",
    "#training  constants\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=learning_rate))\n",
    "\n",
    "train_ds = impressions_ds.take(130_000)\n",
    "test_ds = impressions_ds.skip(130_000).take(10_000)\n",
    "validation_ds = impressions_ds.skip(130_000).skip(10_000)\n",
    "\n",
    "cached_train = train_ds.shuffle(10_000).batch(10000).cache()\n",
    "cached_test = test_ds.batch(1024).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=epochs)\n",
    "\n",
    "\n",
    "model.evaluate(cached_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c359e6d-d831-47ae-b3f9-feb4079c18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'args_1:0' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'args_0:0' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'args_2:0' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'history': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'N10059', b'N54496', b'N871', b'N44559', b'N62342', b'N3909',\n",
      "        b'N30867', b'N32939', b'N10414', b'N31801']], dtype=object)>, 'category': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'news', b'news', b'tv', b'health', b'video', b'finance',\n",
      "        b'news', b'movies', b'movies', b'news']], dtype=object)>, 'subcategory': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'newsus', b'newsus', b'tv-celebrity', b'health-news',\n",
      "        b'science', b'finance-real-estate', b'causes-environment',\n",
      "        b'movies-celebrity', b'movienews', b'newspolitics']], dtype=object)>}. Consider rewriting this model with the Functional API.\n",
      "Recommendations: [b'N36221' b'N29715' b'N28413' b'N21707' b'N62318']\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index_from_dataset(news_ds.batch(100).map(lambda item: (item[\"next_id\"], model.candidate_model(item))))\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index({\n",
    "    \"history\": tf.constant([['N10059', 'N54496', 'N871', 'N44559', 'N62342', 'N3909', 'N30867', 'N32939', 'N10414', 'N31801']]),\n",
    "    \"category\": tf.constant([['news', 'news', 'tv', 'health', 'video', 'finance', 'news', 'movies', 'movies', 'news']]),\n",
    "    \"subcategory\": tf.constant([['newsus', 'newsus', 'tv-celebrity', 'health-news', 'science', 'finance-real-estate', 'causes-environment', 'movies-celebrity', 'movienews', 'newspolitics']])\n",
    "})\n",
    "\n",
    "print(f\"Recommendations: {titles[0, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58451b20-2d65-4a1b-9f7c-24e77e12db82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
