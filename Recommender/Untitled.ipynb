{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a88d079-d831-4475-b93c-cef6f7b70a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "#data import\n",
    "impressions = pd.read_csv(\n",
    "    \"TransformedData.csv\",\n",
    "    header = None,\n",
    "    names= ['user_id','timestamp','history','category','subcategory','title','next_item']\n",
    "    ) \n",
    "\n",
    "news_data = pd.read_table(\"news.tsv\",\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "\n",
    "news_data = news_data.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715b6ec4-de41-4f4f-b4c1-7d44a306510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=(TensorSpec(shape=(3,), dtype=tf.string, name=None), TensorSpec(shape=(3, 10), dtype=tf.string, name=None))>\n"
     ]
    }
   ],
   "source": [
    "history_eval = impressions[\"history\"].map(lambda x: literal_eval(x)).tolist()\n",
    "category_eval = impressions[\"category\"].map(lambda x: literal_eval(x)).tolist()\n",
    "subcategory_eval = impressions[\"subcategory\"].map(lambda x: literal_eval(x)).tolist()\n",
    "next_id_eval = impressions[\"next_item\"].map(lambda x: literal_eval(x)).tolist()\n",
    "\n",
    "#Create Data Tensors && dataset\n",
    "history_tensor = tf.convert_to_tensor(list(zip(history_eval, category_eval, subcategory_eval)), dtype=tf.string)\n",
    "next_news_tensor = tf.convert_to_tensor(next_id_eval, dtype=tf.string)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((next_news_tensor,history_tensor))\n",
    "\n",
    "print(dataset)\n",
    "#Vocabularies\n",
    "news_id_vocabulary = tf.constant(news_data[\"id\"], dtype=tf.string)\n",
    "news_id_vocabulary = tf.data.Dataset.from_tensor_slices((news_id_vocabulary))\n",
    "news_id_vocabulary = news_id_vocabulary.batch(1000)\n",
    "news_id_vocabulary = np.unique(np.concatenate(list(news_id_vocabulary)))\n",
    "\n",
    "news_category_vocabulary = tf.constant(news_data[\"category\"], dtype=tf.string)\n",
    "news_category_vocabulary = tf.data.Dataset.from_tensor_slices((news_category_vocabulary))\n",
    "news_category_vocabulary = news_category_vocabulary.batch(1000)\n",
    "news_category_vocabulary = np.unique(np.concatenate(list(news_category_vocabulary)))\n",
    "\n",
    "news_subcategory_vocabulary = tf.constant(news_data[\"subcategory\"], dtype=tf.string)\n",
    "news_subcategory_vocabulary = tf.data.Dataset.from_tensor_slices((news_subcategory_vocabulary))\n",
    "news_subcategory_vocabulary = news_subcategory_vocabulary.batch(1000)\n",
    "news_subcategory_vocabulary = np.unique(np.concatenate(list(news_subcategory_vocabulary)))\n",
    "\n",
    "news_id = list(news_data[\"id\"].values)\n",
    "news_category = list(news_data[\"category\"].values)\n",
    "news_subcategory = list(news_data[\"subcategory\"].values)\n",
    "newses = tf.convert_to_tensor(list(zip(news_id,news_category,news_subcategory)), dtype=tf.string)\n",
    "news_dataset = tf.data.Dataset.from_tensor_slices((newses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b451ca-2ab6-414e-8d88-dc6f921afc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension=64\n",
    "learning_rate=0.1\n",
    "epochs=3\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Create History Model\n",
    "        self.history_model = tf.keras.Sequential()\n",
    "        self.history_model._name = \"user_history\"\n",
    "        self.history_model.add(tf.keras.layers.StringLookup(vocabulary=news_id_vocabulary, mask_token=None))\n",
    "        self.history_model.add(tf.keras.layers.Embedding(len(news_id_vocabulary)+1, embedding_dimension))\n",
    "        self.history_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "\n",
    "        #Create Category Model\n",
    "        self.category_model = tf.keras.Sequential()\n",
    "        self.category_model._name = \"user_category\"\n",
    "        self.category_model.add(tf.keras.layers.StringLookup(vocabulary=news_category_vocabulary, mask_token=None))\n",
    "        self.category_model.add(tf.keras.layers.Embedding(len(news_category_vocabulary)+1, embedding_dimension))\n",
    "        self.category_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "\n",
    "        #Create SubCategory Model\n",
    "        self.subcategory_model = tf.keras.Sequential()\n",
    "        self.subcategory_model._name = \"user_subcategory\"\n",
    "        self.subcategory_model.add(tf.keras.layers.StringLookup(vocabulary=news_subcategory_vocabulary, mask_token=None))\n",
    "        self.subcategory_model.add(tf.keras.layers.Embedding(len(news_subcategory_vocabulary)+1, embedding_dimension))\n",
    "        self.subcategory_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "\n",
    "    def call(self, features):\n",
    "        return tf.concat([\n",
    "            self.history_model(features[0]),\n",
    "            self.category_model(features[1]),\n",
    "            self.subcategory_model(features[2]),\n",
    "        ], axis = 1)\n",
    "    \n",
    "class NewsModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ID_model\n",
    "        self.NewsId_model = tf.keras.Sequential()\n",
    "        self.NewsId_model._name = \"news_id\"\n",
    "        self.NewsId_model.add(tf.keras.layers.StringLookup(vocabulary=news_id_vocabulary, mask_token=None))\n",
    "        self.NewsId_model.add(tf.keras.layers.Embedding(len(news_id_vocabulary) +1, embedding_dimension))\n",
    "        \n",
    "        # category model\n",
    "        self.news_category_model = tf.keras.Sequential()\n",
    "        self.news_category_model._name = \"news_category\"\n",
    "        self.news_category_model.add(tf.keras.layers.StringLookup(vocabulary=news_category_vocabulary, mask_token=None))\n",
    "        self.news_category_model.add(tf.keras.layers.Embedding(len(news_category_vocabulary) +1, embedding_dimension))\n",
    "        \n",
    "        # subcategory model\n",
    "        self.news_subcategory_model = tf.keras.Sequential()\n",
    "        self.news_subcategory_model._name = \"news_subcategory\"\n",
    "        self.news_subcategory_model.add(tf.keras.layers.StringLookup(vocabulary=news_subcategory_vocabulary, mask_token=None))\n",
    "        self.news_subcategory_model.add(tf.keras.layers.Embedding(len(news_subcategory_vocabulary) +1, embedding_dimension))\n",
    "\n",
    "    def call(self, features):\n",
    "        return tf.concat([\n",
    "            self.NewsId_model(features[0]),\n",
    "            self.news_category_model(features[1]),\n",
    "            self.news_subcategory_model(features[2]),\n",
    "        ], axis = 1)\n",
    "    \n",
    "class Model(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query_model = tf.keras.Sequential([\n",
    "            UserModel(),\n",
    "            tf.keras.layers.Dense(embedding_dimension)\n",
    "\n",
    "        ])\n",
    "        \n",
    "        self.query_model._name = \"query\"\n",
    "        \n",
    "        self.candidate_model = tf.keras.Sequential([\n",
    "            NewsModel(),\n",
    "            tf.keras.layers.Dense(embedding_dimension)\n",
    "\n",
    "        ])\n",
    "        \n",
    "        self.candidate_model._name = \"candidate\"\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates= news_dataset.batch(1024).map(self.candidate_model),\n",
    "                ),\n",
    "            name = \"retrival_task\"\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        candidate_embedding = self.candidate_model(features[0])\n",
    "        query_embedding = self.query_model(features[1])\n",
    "        return self.task(query_embedding, candidate_embedding, compute_metrics=not training)\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88770335-f7c4-40a3-aef3-86d62c30c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "13/13 [==============================] - 5s 42ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 3.1781 - regularization_loss: 0.0000e+00 - total_loss: 3.1781\n",
      "Epoch 2/3\n",
      "13/13 [==============================] - 0s 11ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 3.2986 - regularization_loss: 0.0000e+00 - total_loss: 3.2986\n",
      "Epoch 3/3\n",
      "13/13 [==============================] - 0s 13ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 3.8934 - regularization_loss: 0.0000e+00 - total_loss: 3.8934\n",
      "10/10 [==============================] - 3s 142ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0333 - factorized_top_k/top_5_categorical_accuracy: 0.0667 - factorized_top_k/top_10_categorical_accuracy: 0.1000 - factorized_top_k/top_50_categorical_accuracy: 0.4333 - factorized_top_k/top_100_categorical_accuracy: 0.9000 - loss: 5.8442 - regularization_loss: 0.0000e+00 - total_loss: 5.8442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03333333507180214,\n",
       " 0.06666667014360428,\n",
       " 0.10000000149011612,\n",
       " 0.4333333373069763,\n",
       " 0.8999999761581421,\n",
       " 11.841562271118164,\n",
       " 0,\n",
       " 11.841562271118164]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train Model\n",
    "#training  constants\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=learning_rate))\n",
    "\n",
    "train_ds = dataset.take(130_000)\n",
    "test_ds = dataset.skip(130_000).take(10_000)\n",
    "validation_ds = dataset.skip(130_000).skip(10_000)\n",
    "\n",
    "cached_train = train_ds.shuffle(10_000).batch(10000).cache()\n",
    "cached_test = test_ds.batch(1024).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=epochs)\n",
    "\n",
    "\n",
    "model.evaluate(cached_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384a9985-171a-42bb-9366-1208c80d2ce0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<MapDataset element_spec=TensorSpec(shape=(3, 64), dtype=tf.float32, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m index \u001b[38;5;241m=\u001b[39m tfrs\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mfactorized_top_k\u001b[38;5;241m.\u001b[39mBruteForce(model\u001b[38;5;241m.\u001b[39mquery_model)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# recommends movies out of the entire movies dataset.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnews_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidate_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43midentifier\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Get recommendations.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m _, titles \u001b[38;5;241m=\u001b[39m index(tf\u001b[38;5;241m.\u001b[39mconstant([[\n\u001b[0;32m     13\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN38629\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN50155\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN29177\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN56426\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN63842\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN36565\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN30710\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN43854\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN41229\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN31983\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     14\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentertainment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealth\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtv-celebrity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusicnews\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtv-celebrity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewsus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtv-celebrity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweightloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight-loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwellness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentertainment-celebrity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweightloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m ]]))\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Projects\\Recommender\\venvTensorFlow\\lib\\site-packages\\tensorflow_recommenders\\layers\\factorized_top_k.py:535\u001b[0m, in \u001b[0;36mBruteForce.index\u001b[1;34m(self, candidates, identifiers)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m identifiers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    533\u001b[0m   identifiers \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrange(candidates\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    536\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    537\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe candidates tensor must be 2D (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidates\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m candidates\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m identifiers\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Projects\\Recommender\\venvTensorFlow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Projects\\Recommender\\venvTensorFlow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (<MapDataset element_spec=TensorSpec(shape=(3, 64), dtype=tf.float32, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "identifier = tf.data.Dataset.from_tensor_slices(news_data[\"id\"])\n",
    "\n",
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index(\n",
    "    news_dataset.batch(100).map(model.candidate_model),\n",
    "    identifier\n",
    ")\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index(tf.constant([[\n",
    "    ['N38629', 'N50155', 'N29177', 'N56426', 'N63842', 'N36565', 'N30710', 'N43854', 'N41229', 'N31983'],\n",
    "    ['tv', 'music', 'tv', 'news', 'tv', 'health', 'health', 'health', 'entertainment', 'health'],\n",
    "    ['tv-celebrity', 'musicnews', 'tv-celebrity', 'newsus', 'tv-celebrity', 'weightloss', 'weight-loss', 'wellness', 'entertainment-celebrity', 'weightloss']\n",
    "]]))\n",
    "print(f\"Then Give this: {titles[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb561c-5030-48a6-8425-908d0fc3be77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbe93903c93104c1f24e7db0e9e356274ebeca06d4a68c84469e17e7d280f5c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
