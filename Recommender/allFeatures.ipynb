{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6564c7c-7467-4430-81e4-49c0af5bac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from typing import Dict, Text\n",
    "\n",
    "#data import\n",
    "impressions = pd.read_csv(\n",
    "    \"finalDataB.csv\",\n",
    "    header = None,\n",
    "    names= ['user_id','timestamp','history','category','subcategory','title','next_item']\n",
    "    ) \n",
    "\n",
    "impressions = impressions.drop(columns=['user_id','timestamp'])\n",
    "\n",
    "news_data = pd.read_table(\"news.tsv\",\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'next_id', 'next_category', 'next_subcategory', 'next_title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "\n",
    "news_data = news_data.drop(columns=['abstract','url', 'title_entities','abstract_entities'])\n",
    "news_data = news_data.drop_duplicates('next_id')\n",
    "\n",
    "history = impressions[\"history\"].map(lambda x: literal_eval(x)).tolist()\n",
    "title = impressions[\"title\"].map(lambda x: literal_eval(x)).tolist()\n",
    "category = impressions[\"category\"].map(lambda x: literal_eval(x)).tolist()\n",
    "subcategory = impressions[\"subcategory\"].map(lambda x: literal_eval(x)).tolist()\n",
    "next_id = impressions[\"next_item\"].map(lambda x: literal_eval(x)[0])\n",
    "next_title = impressions[\"next_item\"].map(lambda x: literal_eval(x)[1])\n",
    "next_category = impressions[\"next_item\"].map(lambda x: literal_eval(x)[2])\n",
    "next_subcategory = impressions[\"next_item\"].map(lambda x: literal_eval(x)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6244ef-c76c-4e5e-88c2-d786c6333d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = tf.ragged.constant(history, dtype=tf.string)\n",
    "title = tf.convert_to_tensor(title, dtype=tf.string)\n",
    "category = tf.ragged.constant(category, dtype=tf.string)\n",
    "subcategory = tf.ragged.constant(subcategory, dtype=tf.string)\n",
    "next_id = tf.constant(next_id, dtype=tf.string)\n",
    "next_title = tf.constant(next_title, dtype=tf.string)\n",
    "next_category = tf.constant(next_category, dtype=tf.string)\n",
    "next_subcategory = tf.constant(next_subcategory, dtype=tf.string)\n",
    "\n",
    "news_dict = {name: np.array(value) for name, value in news_data.items()}\n",
    "impressions_dict = {\n",
    "    \"history\" : history,\n",
    "    \"title\": title,\n",
    "    \"category\" : category,\n",
    "    \"subcategory\" : subcategory,\n",
    "    \"next_id\" : next_id,\n",
    "    \"next_title\": next_title,\n",
    "    \"next_category\" : next_category,\n",
    "    \"next_subcategory\" : next_subcategory,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f67905-aa3d-4970-953d-a3130d2362fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c447f92-bff8-4864-95e9-2ead636f2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_ds = tf.data.Dataset.from_tensor_slices(news_dict)\n",
    "impressions_ds = tf.data.Dataset.from_tensor_slices(impressions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7b24c-0278-4deb-b1ce-b8cd1e05a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabularies\n",
    "news_id_vocabulary = np.unique(np.concatenate(list(news_ds.batch(1_000).map(lambda x: x[\"next_id\"]))))\n",
    "news_title_vocabulary = np.unique(np.concatenate(list(news_ds.batch(1_000).map(lambda x: x[\"next_title\"]))))\n",
    "news_category_vocabulary = np.unique(np.concatenate(list(news_ds.batch(1_000).map(lambda x: x[\"next_category\"]))))\n",
    "news_subcategory_vocabulary = np.unique(np.concatenate(list(news_ds.batch(1_000).map(lambda x: x[\"next_subcategory\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439cbc7-701f-4ab8-bd4f-c8cac7385589",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_ds = news_ds.map(lambda x: {\n",
    "    \"next_id\": x['next_id'],\n",
    "    \"next_title\": x['next_title'],\n",
    "    \"next_category\": x['next_category'],\n",
    "    \"next_subcategory\": x['next_subcategory'],\n",
    "})\n",
    "\n",
    "impressions_ds = impressions_ds.map(lambda x: {\n",
    "    \"history\" : x[\"history\"],\n",
    "    \"title\": x[\"title\"],\n",
    "    \"category\" : x[\"category\"],\n",
    "    \"subcategory\" : x[\"subcategory\"],\n",
    "    \"next_id\" : x[\"next_id\"],\n",
    "    \"next_title\" : x[\"next_title\"],\n",
    "    \"next_category\" : x[\"next_category\"],\n",
    "    \"next_subcategory\" : x[\"next_subcategory\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9651fc-36d2-4c9e-b4b2-422dbfd81f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension=128\n",
    "learning_rate=0.1\n",
    "epochs=30\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        max_tokens = 5_000\n",
    "        \n",
    "        #Create History Model\n",
    "        self.history_model = tf.keras.Sequential()\n",
    "        self.history_model._name = \"user_history\"\n",
    "        self.history_model.add(tf.keras.layers.StringLookup(vocabulary=news_id_vocabulary, mask_token=None))\n",
    "        self.history_model.add(tf.keras.layers.Embedding(len(news_id_vocabulary)+1, embedding_dimension))\n",
    "        self.history_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "        \n",
    "        #Create Title Model\n",
    "        \n",
    "        self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_tokens)\n",
    "\n",
    "        self.title_model = tf.keras.Sequential()\n",
    "        self.title_model._name = \"user_title\"\n",
    "        self.title_model.add(self.title_vectorizer)\n",
    "        self.title_model.add(tf.keras.layers.Embedding(max_tokens, embedding_dimension, mask_zero=True))\n",
    "        self.title_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "        \n",
    "        \n",
    "        self.title_vectorizer.adapt(news_title_vocabulary)\n",
    "\n",
    "        #Create Category Model\n",
    "        self.category_model = tf.keras.Sequential()\n",
    "        self.category_model._name = \"user_category\"\n",
    "        self.category_model.add(tf.keras.layers.StringLookup(vocabulary=news_category_vocabulary, mask_token=None))\n",
    "        self.category_model.add(tf.keras.layers.Embedding(len(news_category_vocabulary)+1, embedding_dimension))\n",
    "        self.category_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "\n",
    "        #Create SubCategory Model\n",
    "        self.subcategory_model = tf.keras.Sequential()\n",
    "        self.subcategory_model._name = \"user_subcategory\"\n",
    "        self.subcategory_model.add(tf.keras.layers.StringLookup(vocabulary=news_subcategory_vocabulary, mask_token=None))\n",
    "        self.subcategory_model.add(tf.keras.layers.Embedding(len(news_subcategory_vocabulary)+1, embedding_dimension))\n",
    "        self.subcategory_model.add(tf.keras.layers.GRU(embedding_dimension))\n",
    "\n",
    "    def call(self, features) -> tf.Tensor:\n",
    "        return tf.concat([\n",
    "            self.history_model(features[\"history\"]),\n",
    "            self.title_model(features[\"title\"]),\n",
    "            self.category_model(features[\"category\"]),\n",
    "            self.subcategory_model(features[\"subcategory\"]),\n",
    "        ], axis = 1)\n",
    "    \n",
    "class NewsModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        max_tokens = 5_000\n",
    "        \n",
    "        # ID_model\n",
    "        self.NewsId_model = tf.keras.Sequential()\n",
    "        self.NewsId_model._name = \"news_id\"\n",
    "        self.NewsId_model.add(tf.keras.layers.StringLookup(vocabulary=news_id_vocabulary, mask_token=None))\n",
    "        self.NewsId_model.add(tf.keras.layers.Embedding(len(news_id_vocabulary) +1, embedding_dimension))\n",
    "        \n",
    "        #title model\n",
    "        \n",
    "        self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_tokens)\n",
    "\n",
    "        self.news_title_model = tf.keras.Sequential()\n",
    "        self.news_title_model._name = \"news_title\"\n",
    "        self.news_title_model.add(self.title_vectorizer)\n",
    "        self.news_title_model.add(tf.keras.layers.Embedding(max_tokens, embedding_dimension, mask_zero=True))\n",
    "        self.news_title_model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "        \n",
    "        \n",
    "        self.title_vectorizer.adapt(news_title_vocabulary)\n",
    "        \n",
    "        # category model\n",
    "        self.news_category_model = tf.keras.Sequential()\n",
    "        self.news_category_model._name = \"news_category\"\n",
    "        self.news_category_model.add(tf.keras.layers.StringLookup(vocabulary=news_category_vocabulary, mask_token=None))\n",
    "        self.news_category_model.add(tf.keras.layers.Embedding(len(news_category_vocabulary) +1, embedding_dimension))\n",
    "        \n",
    "        # subcategory model\n",
    "        self.news_subcategory_model = tf.keras.Sequential()\n",
    "        self.news_subcategory_model._name = \"news_subcategory\"\n",
    "        self.news_subcategory_model.add(tf.keras.layers.StringLookup(vocabulary=news_subcategory_vocabulary, mask_token=None))\n",
    "        self.news_subcategory_model.add(tf.keras.layers.Embedding(len(news_subcategory_vocabulary) +1, embedding_dimension))\n",
    "\n",
    "    def call(self, features) -> tf.Tensor:\n",
    "        return tf.concat([\n",
    "            self.NewsId_model(features[\"next_id\"]),\n",
    "            self.news_title_model(features[\"next_title\"]),\n",
    "            self.news_category_model(features[\"next_category\"]),\n",
    "            self.news_subcategory_model(features[\"next_subcategory\"]),\n",
    "        ], axis = 1)\n",
    "    \n",
    "class Model(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query_model = tf.keras.Sequential([\n",
    "            UserModel(),\n",
    "            tf.keras.layers.Dense(embedding_dimension),\n",
    "        ])\n",
    "        \n",
    "        self.query_model._name = \"query\"\n",
    "        \n",
    "        self.candidate_model = tf.keras.Sequential([\n",
    "            NewsModel(),\n",
    "            tf.keras.layers.Dense(embedding_dimension),\n",
    "        ])\n",
    "        \n",
    "        self.candidate_model._name = \"candidate\"\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates= news_ds.batch(1024).map(self.candidate_model),\n",
    "                ),\n",
    "            name = \"retrival_task\"\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        candidate_embedding = self.candidate_model({\n",
    "            \"next_id\": features[\"next_id\"],\n",
    "            \"next_title\": features[\"next_title\"],\n",
    "            \"next_category\":features[\"next_category\"],\n",
    "            \"next_subcategory\": features[\"next_subcategory\"],\n",
    "        })\n",
    "        query_embedding = self.query_model({\n",
    "            \"history\": features[\"history\"],\n",
    "            \"title\": features[\"title\"],\n",
    "            \"category\":features[\"category\"],\n",
    "            \"subcategory\": features[\"subcategory\"],\n",
    "        })\n",
    "        return self.task(query_embedding, candidate_embedding, compute_metrics=not training)\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "25ce9263-8de8-4585-85b0-1c4ae0261ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'next_title': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'history': <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=string>, 'title': <tf.Tensor 'IteratorGetNext:7' shape=(None, 10, 1) dtype=string>, 'category': <tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=string>, 'subcategory': <tf.Tensor 'IteratorGetNext:6' shape=(None, None) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'next_title': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'history': <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=string>, 'title': <tf.Tensor 'IteratorGetNext:7' shape=(None, 10, 1) dtype=string>, 'category': <tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=string>, 'subcategory': <tf.Tensor 'IteratorGetNext:6' shape=(None, None) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "13/13 [==============================] - 26s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 21905780.5781 - regularization_loss: 0.0000e+00 - total_loss: 21905780.5781\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 12s 934ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 4907244.0000 - regularization_loss: 0.0000e+00 - total_loss: 4907244.0000\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 13s 975ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 1006636.8616 - regularization_loss: 0.0000e+00 - total_loss: 1006636.8616\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 12s 947ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 485166.7522 - regularization_loss: 0.0000e+00 - total_loss: 485166.7522\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 12s 947ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 368060.8013 - regularization_loss: 0.0000e+00 - total_loss: 368060.8013\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 12s 949ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 295233.3783 - regularization_loss: 0.0000e+00 - total_loss: 295233.3783\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 13s 964ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 195237.6105 - regularization_loss: 0.0000e+00 - total_loss: 195237.6105\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 13s 975ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 149974.9989 - regularization_loss: 0.0000e+00 - total_loss: 149974.9989\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 12s 931ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 129736.4615 - regularization_loss: 0.0000e+00 - total_loss: 129736.4615\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 12s 936ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 125862.6484 - regularization_loss: 0.0000e+00 - total_loss: 125862.6484\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 12s 936ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 115846.8940 - regularization_loss: 0.0000e+00 - total_loss: 115846.8940\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 12s 929ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 110079.4609 - regularization_loss: 0.0000e+00 - total_loss: 110079.4609\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 12s 932ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 108838.4939 - regularization_loss: 0.0000e+00 - total_loss: 108838.4939\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 12s 935ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 105052.0022 - regularization_loss: 0.0000e+00 - total_loss: 105052.0022\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 12s 932ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 103279.1133 - regularization_loss: 0.0000e+00 - total_loss: 103279.1133\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 12s 934ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 101564.5179 - regularization_loss: 0.0000e+00 - total_loss: 101564.5179\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 12s 932ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 100632.3984 - regularization_loss: 0.0000e+00 - total_loss: 100632.3984\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 12s 961ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 98985.4715 - regularization_loss: 0.0000e+00 - total_loss: 98985.4715\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 12s 928ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 98437.2288 - regularization_loss: 0.0000e+00 - total_loss: 98437.2288\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 12s 931ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 97776.9185 - regularization_loss: 0.0000e+00 - total_loss: 97776.9185\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 12s 935ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 96837.6959 - regularization_loss: 0.0000e+00 - total_loss: 96837.6959\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 12s 941ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 96078.2472 - regularization_loss: 0.0000e+00 - total_loss: 96078.2472\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 12s 949ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 95750.5229 - regularization_loss: 0.0000e+00 - total_loss: 95750.5229\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 12s 946ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 95345.6356 - regularization_loss: 0.0000e+00 - total_loss: 95345.6356\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 12s 936ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 95057.8912 - regularization_loss: 0.0000e+00 - total_loss: 95057.8912\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 12s 942ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 94472.8298 - regularization_loss: 0.0000e+00 - total_loss: 94472.8298\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 12s 942ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 94178.2790 - regularization_loss: 0.0000e+00 - total_loss: 94178.2790\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 12s 963ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 94093.3465 - regularization_loss: 0.0000e+00 - total_loss: 94093.3465\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 12s 940ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 94192.6864 - regularization_loss: 0.0000e+00 - total_loss: 94192.6864\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 12s 941ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 93563.0011 - regularization_loss: 0.0000e+00 - total_loss: 93563.0011\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'next_title': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'history': <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=string>, 'title': <tf.Tensor 'IteratorGetNext:7' shape=(None, 10, 1) dtype=string>, 'category': <tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=string>, 'subcategory': <tf.Tensor 'IteratorGetNext:6' shape=(None, None) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "10/10 [==============================] - 19s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 1.0000e-04 - factorized_top_k/top_10_categorical_accuracy: 2.0000e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0052 - factorized_top_k/top_100_categorical_accuracy: 0.0149 - loss: 7364.0813 - regularization_loss: 0.0000e+00 - total_loss: 7364.0813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 9.999999747378752e-05,\n",
       " 0.00019999999494757503,\n",
       " 0.005200000014156103,\n",
       " 0.01489999983459711,\n",
       " 5724.77587890625,\n",
       " 0,\n",
       " 5724.77587890625]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train Model\n",
    "#training  constants\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=learning_rate))\n",
    "\n",
    "train_ds = impressions_ds.take(130_000)\n",
    "test_ds = impressions_ds.skip(130_000).take(10_000)\n",
    "validation_ds = impressions_ds.skip(130_000).skip(10_000)\n",
    "\n",
    "cached_train = train_ds.shuffle(10_000).batch(10000).cache()\n",
    "cached_test = test_ds.batch(1024).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=epochs)\n",
    "\n",
    "\n",
    "model.evaluate(cached_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8c359e6d-d831-47ae-b3f9-feb4079c18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'next_id': <tf.Tensor 'args_1:0' shape=(None,) dtype=string>, 'next_title': <tf.Tensor 'args_3:0' shape=(None,) dtype=string>, 'next_category': <tf.Tensor 'args_0:0' shape=(None,) dtype=string>, 'next_subcategory': <tf.Tensor 'args_2:0' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'history': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'N11948', b'N39074', b'N52066', b'N13233', b'N20489', b'N33513',\n",
      "        b'N54496', b'N28818', b'N25114', b'N986']], dtype=object)>, 'title': <tf.Tensor: shape=(1, 10, 1), dtype=string, numpy=\n",
      "array([[[b\"Ohio State thinks it's cute that Florida, LSU arguing over DBU\"],\n",
      "        [b'Eliud Kipchoge runs 1:59 marathon, first to break 2 hours'],\n",
      "        [b'Elton John Defends Ellen DeGeneres Over George W. Bush Friendship'],\n",
      "        [b'Carlos Correa lost his mind while saving Astros vs. Yankees in ALCS'],\n",
      "        [b'Australian wrongly jailed for 19 years wins payout'],\n",
      "        [b'Trace Adkins ties the knot in New Orleans'],\n",
      "        [b'Matt Lauer allegations: Megyn Kelly lauds Meredith Vieira, Ann Curry'],\n",
      "        [b\"Man mistakenly ID'd by Browns in beer-dumping incident sues team\"],\n",
      "        [b\"There's a mouse hiding among mushrooms in this viral brainteaser. Can you spot it?\"],\n",
      "        [b\"Ken Fisher's sexist comments have cost his company nearly $1 billion in assets\"]]],\n",
      "      dtype=object)>, 'category': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'sports', b'sports', b'music', b'sports', b'news', b'music',\n",
      "        b'news', b'sports', b'lifestyle', b'finance']], dtype=object)>, 'subcategory': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'football_ncaa', b'more_sports', b'music-celebrity',\n",
      "        b'baseball_mlb', b'newsworld', b'music-celebrity', b'newsus',\n",
      "        b'football_nfl', b'lifestylebuzz', b'finance-companies']],\n",
      "      dtype=object)>}. Consider rewriting this model with the Functional API.\n",
      "Recommendations: [b'N26336' b'N60727' b'N6760' b'N44831' b'N40342']\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index_from_dataset(news_ds.batch(100).map(lambda item: (item[\"next_id\"], model.candidate_model(item))))\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index({\n",
    "    \"history\": tf.constant([['N11948', 'N39074', 'N52066', 'N13233', 'N20489', 'N33513', 'N54496', 'N28818', 'N25114', 'N986']]),\n",
    "    \"title\": tf.constant([[[\"Ohio State thinks it's cute that Florida, LSU arguing over DBU\"], ['Eliud Kipchoge runs 1:59 marathon, first to break 2 hours'], ['Elton John Defends Ellen DeGeneres Over George W. Bush Friendship'], ['Carlos Correa lost his mind while saving Astros vs. Yankees in ALCS'], ['Australian wrongly jailed for 19 years wins payout'], ['Trace Adkins ties the knot in New Orleans'], ['Matt Lauer allegations: Megyn Kelly lauds Meredith Vieira, Ann Curry'], [\"Man mistakenly ID'd by Browns in beer-dumping incident sues team\"], [\"There's a mouse hiding among mushrooms in this viral brainteaser. Can you spot it?\"], [\"Ken Fisher's sexist comments have cost his company nearly $1 billion in assets\"]]]),\n",
    "    \"category\": tf.constant([['sports', 'sports', 'music', 'sports', 'news', 'music', 'news', 'sports', 'lifestyle', 'finance']]),\n",
    "    \"subcategory\": tf.constant([['football_ncaa', 'more_sports', 'music-celebrity', 'baseball_mlb', 'newsworld', 'music-celebrity', 'newsus', 'football_nfl', 'lifestylebuzz', 'finance-companies']])\n",
    "})\n",
    "\n",
    "print(f\"Recommendations: {titles[0, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58451b20-2d65-4a1b-9f7c-24e77e12db82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
